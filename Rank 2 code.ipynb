{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('DStrain.csv')\n",
    "test = pd.read_csv('DStest.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['num_missing'] = train.isnull().sum(axis=1)\n",
    "test['num_missing'] = test.isnull().sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "missing_columns = ['gender','enrolled_university','education_level','major_discipline','experience','company_size','company_type','last_new_job']\n",
    "for col in missing_columns:\n",
    "    train[col].fillna('NA',inplace=True)\n",
    "    test[col].fillna('NA',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating features from city\n",
      "Creating features from gender\n",
      "Creating features from relevent_experience\n",
      "Creating features from enrolled_university\n",
      "Creating features from education_level\n",
      "Creating features from major_discipline\n",
      "Creating features from experience\n",
      "Creating features from company_size\n",
      "Creating features from company_type\n",
      "Creating features from last_new_job\n"
     ]
    }
   ],
   "source": [
    "#FEATURE ENGINEERING - based on higher than mean and median of training hours within each category\n",
    "def th_features(column,train=train,test=test):\n",
    "    gp_median = train.groupby(column)['training_hours'].median().reset_index().rename(columns={'training_hours':column+'_th_median'})\n",
    "    gp_mean = train.groupby(column)['training_hours'].mean().reset_index().rename(columns={'training_hours':column+'_th_mean'})\n",
    "    train = train.merge(gp_median,on=column,how='left')\n",
    "    train = train.merge(gp_mean,on=column,how='left')\n",
    "    test = test.merge(gp_median,on=column,how='left')\n",
    "    test = test.merge(gp_mean,on=column,how='left')\n",
    "    train[column+'_higher_th_median_ind'] = train.training_hours > train[column+'_th_median']\n",
    "    train[column+'_higher_th_mean_ind'] = train.training_hours > train[column+'_th_mean']\n",
    "    test[column+'_higher_th_median_ind'] = test.training_hours > test[column+'_th_median']\n",
    "    test[column+'_higher_th_mean_ind'] = test.training_hours > test[column+'_th_mean']\n",
    "    train.drop([column+'_th_mean',column+'_th_median'],axis=1,inplace=True)\n",
    "    test.drop([column+'_th_mean',column+'_th_median'],axis=1,inplace=True)\n",
    "    return train,test\n",
    "\n",
    "def count_features(column,train=train,test=test):\n",
    "    gp_count = train.groupby(column)['city_development_index'].count().reset_index().rename(columns={'city_development_index':column+'_count'})\n",
    "    train = train.merge(gp_count,on=column,how='left')\n",
    "    test = test.merge(gp_count,on=column,how='left')\n",
    "    return train,test\n",
    "\n",
    "def cdi_features(column,train=train,test=test):\n",
    "    gp_median = train.groupby(column)['city_development_index'].median().reset_index().rename(columns={'city_development_index':column+'_cdi_median'})\n",
    "    gp_mean = train.groupby(column)['city_development_index'].mean().reset_index().rename(columns={'city_development_index':column+'_cdi_mean'})\n",
    "    gp_sd = train.groupby(column)['city_development_index'].std().reset_index().rename(columns={'city_development_index':column+'_cdi_sd'})\n",
    "    gp_skew = train.groupby(column)['city_development_index'].skew().reset_index().rename(columns={'city_development_index':column+'_cdi_skew'})\n",
    "    gp_kurtosis = train.groupby(column)['city_development_index'].apply(lambda x:x.kurtosis()).reset_index().rename(columns={'city_development_index':column+'_cdi_kurtosis'})\n",
    "    train = train.merge(gp_median,on=column,how='left')\n",
    "    train = train.merge(gp_mean,on=column,how='left')\n",
    "    train = train.merge(gp_sd,on=column,how='left')\n",
    "    train = train.merge(gp_skew,on=column,how='left')\n",
    "    train = train.merge(gp_kurtosis,on=column,how='left')\n",
    "    test = test.merge(gp_median,on=column,how='left')\n",
    "    test = test.merge(gp_mean,on=column,how='left')\n",
    "    test = test.merge(gp_sd,on=column,how='left')\n",
    "    test = test.merge(gp_skew,on=column,how='left')\n",
    "    test = test.merge(gp_kurtosis,on=column,how='left')\n",
    "    train[column+'_higher_cdi_median_ind'] = train.city_development_index > train[column+'_cdi_median']\n",
    "    test[column+'_higher_cdi_median_ind'] = test.city_development_index > test[column+'_cdi_median']\n",
    "    train[column+'_higher_cdi_mean_ind'] = train.city_development_index > train[column+'_cdi_mean']\n",
    "    test[column+'_higher_cdi_mean_ind'] = test.city_development_index > test[column+'_cdi_mean']\n",
    "    return train,test\n",
    "\n",
    "def mean_features(column,train=train,test=test):\n",
    "    gp_mean = train.groupby(column)['target'].mean().reset_index().rename(columns={'target':column+'_mean'})\n",
    "    train = train.merge(gp_mean,on=column,how='left')\n",
    "    test = test.merge(gp_mean,on=column,how='left')\n",
    "    return train,test\n",
    "\n",
    "def preferred_features(column,train=train,test=test):\n",
    "    preferred = train.groupby('city')[column].apply(lambda x: x.mode()).reset_index().rename(columns={column:'preferred_'+column})\n",
    "    preferred = preferred[preferred.level_1 == 0].drop(['level_1'],axis=1)\n",
    "    train = train.merge(preferred,on='city',how='left')\n",
    "    test = test.merge(preferred,on='city',how='left')\n",
    "    return train,test\n",
    "\n",
    "categorical_columns = ['city','gender','relevent_experience','enrolled_university','education_level','major_discipline','experience','company_size','company_type','last_new_job']\n",
    "new_categorical_columns = []\n",
    "for column in categorical_columns:\n",
    "    print('Creating features from ' + column)\n",
    "    train,test = th_features(column,train,test)\n",
    "    train,test = count_features(column,train,test)\n",
    "    train,test = mean_features(column,train,test)\n",
    "    new_categorical_columns.append(column+'_higher_th_median_ind')\n",
    "    new_categorical_columns.append(column+'_higher_th_mean_ind')\n",
    "    if column != 'city':\n",
    "        train,test = cdi_features(column,train,test)\n",
    "        train,test = preferred_features(column,train,test)\n",
    "        new_categorical_columns.append(column+'_higher_cdi_median_ind')\n",
    "        new_categorical_columns.append(column+'_higher_cdi_mean_ind')\n",
    "        new_categorical_columns.append('preferred_'+column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming city\n",
      "Transforming gender\n",
      "Transforming relevent_experience\n",
      "Transforming enrolled_university\n",
      "Transforming education_level\n",
      "Transforming major_discipline\n",
      "Transforming experience\n",
      "Transforming company_size\n",
      "Transforming company_type\n",
      "Transforming last_new_job\n",
      "Transforming city_higher_th_median_ind\n",
      "Transforming city_higher_th_mean_ind\n",
      "Transforming gender_higher_th_median_ind\n",
      "Transforming gender_higher_th_mean_ind\n",
      "Transforming gender_higher_cdi_median_ind\n",
      "Transforming gender_higher_cdi_mean_ind\n",
      "Transforming preferred_gender\n",
      "Transforming relevent_experience_higher_th_median_ind\n",
      "Transforming relevent_experience_higher_th_mean_ind\n",
      "Transforming relevent_experience_higher_cdi_median_ind\n",
      "Transforming relevent_experience_higher_cdi_mean_ind\n",
      "Transforming preferred_relevent_experience\n",
      "Transforming enrolled_university_higher_th_median_ind\n",
      "Transforming enrolled_university_higher_th_mean_ind\n",
      "Transforming enrolled_university_higher_cdi_median_ind\n",
      "Transforming enrolled_university_higher_cdi_mean_ind\n",
      "Transforming preferred_enrolled_university\n",
      "Transforming education_level_higher_th_median_ind\n",
      "Transforming education_level_higher_th_mean_ind\n",
      "Transforming education_level_higher_cdi_median_ind\n",
      "Transforming education_level_higher_cdi_mean_ind\n",
      "Transforming preferred_education_level\n",
      "Transforming major_discipline_higher_th_median_ind\n",
      "Transforming major_discipline_higher_th_mean_ind\n",
      "Transforming major_discipline_higher_cdi_median_ind\n",
      "Transforming major_discipline_higher_cdi_mean_ind\n",
      "Transforming preferred_major_discipline\n",
      "Transforming experience_higher_th_median_ind\n",
      "Transforming experience_higher_th_mean_ind\n",
      "Transforming experience_higher_cdi_median_ind\n",
      "Transforming experience_higher_cdi_mean_ind\n",
      "Transforming preferred_experience\n",
      "Transforming company_size_higher_th_median_ind\n",
      "Transforming company_size_higher_th_mean_ind\n",
      "Transforming company_size_higher_cdi_median_ind\n",
      "Transforming company_size_higher_cdi_mean_ind\n",
      "Transforming preferred_company_size\n",
      "Transforming company_type_higher_th_median_ind\n",
      "Transforming company_type_higher_th_mean_ind\n",
      "Transforming company_type_higher_cdi_median_ind\n",
      "Transforming company_type_higher_cdi_mean_ind\n",
      "Transforming preferred_company_type\n",
      "Transforming last_new_job_higher_th_median_ind\n",
      "Transforming last_new_job_higher_th_mean_ind\n",
      "Transforming last_new_job_higher_cdi_median_ind\n",
      "Transforming last_new_job_higher_cdi_mean_ind\n",
      "Transforming preferred_last_new_job\n"
     ]
    }
   ],
   "source": [
    "train.fillna(0,inplace=True)\n",
    "test.fillna(0,inplace=True)\n",
    "categorical_columns = categorical_columns + new_categorical_columns\n",
    "for col in categorical_columns:\n",
    "    train[col] = train[col].astype('category')\n",
    "    test[col] = test[col].astype('category')\n",
    "    print('Transforming ' + col)\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(train[col].append(test[col]).astype(str))\n",
    "    train[col] = encoder.transform(train[col].astype(str))\n",
    "    test[col] = encoder.transform(test[col].astype(str))\n",
    "predictor_columns = list(train.columns.values)\n",
    "predictor_columns.remove('enrollee_id')\n",
    "predictor_columns.remove('target')\n",
    "target_columns = 'target'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rounds = 5000\n",
    "early_stop_rounds = 200\n",
    "lgbm_params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric' : 'auc',\n",
    "    'num_leaves' : 31,\n",
    "    'max_depth': 6,\n",
    "    'learning_rate' : 0.01,\n",
    "    'feature_fraction' : 0.25,\n",
    "    'bagging_fraction' : 0.6,\n",
    "    'bagging_freq' : 20,\n",
    "    'verbosity' : 0,\n",
    "    'num_threads' : 8,\n",
    "    'min_data_in_leaf' : 15,\n",
    "    'lambda_l1' : 1.2,\n",
    "    'lambda_l2' : 0.9,\n",
    "    'cat_smooth' : 100,\n",
    "    'max_bin' : 25,\n",
    "    'min_gain_to_split' : 0.005,\n",
    "    'max_cat_to_onehot' : 200,\n",
    "    'scale_pos_weight' : 6\n",
    "}\n",
    "\n",
    "num_folds = 5\n",
    "kf = KFold(n_splits=num_folds,shuffle=True,random_state=37)\n",
    "kf.get_n_splits(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training on fold: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BSK TEJA\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:1036: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\BSK TEJA\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:681: UserWarning: categorical_feature in param dict is overridden.\n",
      "  warnings.warn('categorical_feature in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training on fold: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BSK TEJA\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:1036: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\BSK TEJA\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:681: UserWarning: categorical_feature in param dict is overridden.\n",
      "  warnings.warn('categorical_feature in param dict is overridden.')\n",
      "C:\\Users\\BSK TEJA\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:681: UserWarning: categorical_feature in param dict is overridden.\n",
      "  warnings.warn('categorical_feature in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training on fold: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BSK TEJA\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:1036: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\BSK TEJA\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:681: UserWarning: categorical_feature in param dict is overridden.\n",
      "  warnings.warn('categorical_feature in param dict is overridden.')\n",
      "C:\\Users\\BSK TEJA\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:681: UserWarning: categorical_feature in param dict is overridden.\n",
      "  warnings.warn('categorical_feature in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training on fold: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BSK TEJA\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:1036: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\BSK TEJA\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:681: UserWarning: categorical_feature in param dict is overridden.\n",
      "  warnings.warn('categorical_feature in param dict is overridden.')\n",
      "C:\\Users\\BSK TEJA\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:681: UserWarning: categorical_feature in param dict is overridden.\n",
      "  warnings.warn('categorical_feature in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training on fold: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BSK TEJA\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:1036: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\BSK TEJA\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:681: UserWarning: categorical_feature in param dict is overridden.\n",
      "  warnings.warn('categorical_feature in param dict is overridden.')\n",
      "C:\\Users\\BSK TEJA\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:681: UserWarning: categorical_feature in param dict is overridden.\n",
      "  warnings.warn('categorical_feature in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:0.7363513341943972+-0.012442435205885505 Valid:0.6781720831835331+-0.004280347829272982\n"
     ]
    }
   ],
   "source": [
    "import collections as cl\n",
    "results_train = []\n",
    "results_valid = []\n",
    "feature_importance = dict()\n",
    "for i, (train_index, test_index) in zip(range(1,num_folds+1),kf.split(train)):\n",
    "    X_train = train.loc[train_index,predictor_columns].values\n",
    "    X_test = train.loc[test_index,predictor_columns].values\n",
    "    y_train = train.loc[train_index,target_columns].values\n",
    "    y_test = train.loc[test_index,target_columns].values\n",
    "    X_train = lgb.Dataset(X_train,y_train,feature_name=predictor_columns,categorical_feature = categorical_columns)\n",
    "    X_test = lgb.Dataset(X_test,y_test,feature_name=predictor_columns,categorical_feature = categorical_columns)\n",
    "    gc.collect()\n",
    "    print('Starting training on fold:',i)\n",
    "    model = lgb.train(lgbm_params,X_train,num_boost_round=rounds,valid_sets=[X_train,X_test],valid_names=['train','valid'],\n",
    "                    early_stopping_rounds=early_stop_rounds,verbose_eval=0,)\n",
    "    results_train.append(model.best_score['train']['auc'])\n",
    "    results_valid.append(model.best_score['valid']['auc'])\n",
    "    test['target_' + str(i)] = model.predict(test.loc[:,predictor_columns].values)\n",
    "    total_gain = np.sum(model.feature_importance('gain'))\n",
    "    if i == 1:\n",
    "        for feature, importance in zip(model.feature_name(),model.feature_importance('gain')):\n",
    "            feature_importance.update({feature : importance/total_gain})\n",
    "    else:\n",
    "        for feature, importance in zip(model.feature_name(),model.feature_importance('gain')):\n",
    "            feature_importance[feature] = feature_importance[feature] + (importance/total_gain)\n",
    "for feature,importance in zip(feature_importance.keys(),feature_importance.values()):\n",
    "    feature_importance[feature] = 100*importance/num_folds\n",
    "feature_importance = cl.OrderedDict(sorted(feature_importance.items(), key=lambda t: t[1], reverse=True))\n",
    "print('Train:' + str(np.mean(results_train)) + '+-' + str(np.std(results_train)), 'Valid:' + str(np.mean(results_valid)) + '+-' + str(np.std(results_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test['target'] = 0\n",
    "for i in range(1,num_folds+1):\n",
    "    test['target_'+str(i)] = test['target_'+str(i)].rank(pct=True)\n",
    "    test['target'] = test['target'] + test['target_'+str(i)]\n",
    "test['target'] = test['target']/float(num_folds)\n",
    "test[['enrollee_id','target']].to_csv('sub_sam_lgb.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training on fold: 1\n",
      "Starting training on fold: 2\n",
      "Starting training on fold: 3\n",
      "Starting training on fold: 4\n",
      "Starting training on fold: 5\n",
      "Train:0.7831937275191583+-0.0007555438966608904 Valid:0.6755802231334923+-0.007591761707823492\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "num_folds = 5\n",
    "kf = KFold(n_splits=num_folds,shuffle=True,random_state=37)\n",
    "kf.get_n_splits(train)\n",
    "X = pd.get_dummies(columns=categorical_columns,data=train[predictor_columns],drop_first=True)\n",
    "X_t = pd.get_dummies(columns=categorical_columns,data=test[predictor_columns],drop_first=True)\n",
    "results_train = []\n",
    "results_valid = []\n",
    "for i, (train_index, test_index) in zip(range(1,num_folds+1),kf.split(train)):\n",
    "    X_train = X.loc[train_index,:].values\n",
    "    X_test = X.loc[test_index,:].values\n",
    "    y_train = train.loc[train_index,target_columns].values\n",
    "    y_test = train.loc[test_index,target_columns].values\n",
    "    gc.collect()\n",
    "    print('Starting training on fold:',i)\n",
    "    model = RandomForestClassifier(n_estimators=2000, criterion='entropy', max_depth=8, min_samples_split=10, \n",
    "                                   min_samples_leaf=5,max_features=25,n_jobs=-1,random_state=37,bootstrap=True)\n",
    "    model.fit(X=X_train,y=y_train)\n",
    "    results_train.append(roc_auc_score(y_train,model.predict_proba(X_train)[:,1]))\n",
    "    results_valid.append(roc_auc_score(y_test,model.predict_proba(X_test)[:,1]))\n",
    "    test['target_rf_' + str(i)] = model.predict_proba(X_t.values)[:,1]\n",
    "print('Train:' + str(np.mean(results_train)) + '+-' + str(np.std(results_train)), 'Valid:' + str(np.mean(results_valid)) + '+-' + str(np.std(results_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test['target'] = 0\n",
    "for i in range(1,num_folds+1):\n",
    "    test['target_rf'+str(i)] = test['target_rf_'+str(i)].rank(pct=True)\n",
    "    test['target'] = test['target'] + test['target_rf_'+str(i)]\n",
    "test['target'] = test['target']/float(num_folds)\n",
    "test[['enrollee_id','target']].to_csv('sub_sam_rf.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training on fold: 1\n",
      "Starting training on fold: 2\n",
      "Starting training on fold: 3\n",
      "Starting training on fold: 4\n",
      "Starting training on fold: 5\n",
      "Train:0.7581600538234055+-0.0020969330747042402 Valid:0.6746285776006717+-0.00573772755124784\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "num_folds = 5\n",
    "kf = KFold(n_splits=num_folds,shuffle=True,random_state=37)\n",
    "kf.get_n_splits(train)\n",
    "X = pd.get_dummies(columns=categorical_columns,data=train[predictor_columns],drop_first=True)\n",
    "X_t = pd.get_dummies(columns=categorical_columns,data=test[predictor_columns],drop_first=True)\n",
    "results_train = []\n",
    "results_valid = []\n",
    "for i, (train_index, test_index) in zip(range(1,num_folds+1),kf.split(train)):\n",
    "    X_train = X.loc[train_index,:].values\n",
    "    X_test = X.loc[test_index,:].values\n",
    "    y_train = train.loc[train_index,target_columns].values\n",
    "    y_test = train.loc[test_index,target_columns].values\n",
    "    gc.collect()\n",
    "    print('Starting training on fold:',i)\n",
    "    model = ExtraTreesClassifier(n_estimators=3000, criterion='entropy', max_depth=8, min_samples_split=50, \n",
    "                                 min_samples_leaf=15,max_features=300,min_impurity_decrease=0.0, \n",
    "                                 bootstrap=True, n_jobs=-1, random_state=37)\n",
    "    model.fit(X=X_train,y=y_train)\n",
    "    results_train.append(roc_auc_score(y_train,model.predict_proba(X_train)[:,1]))\n",
    "    results_valid.append(roc_auc_score(y_test,model.predict_proba(X_test)[:,1]))\n",
    "    test['target_et_' + str(i)] = model.predict_proba(X_t.values)[:,1]\n",
    "print('Train:' + str(np.mean(results_train)) + '+-' + str(np.std(results_train)), 'Valid:' + str(np.mean(results_valid)) + '+-' + str(np.std(results_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test['target'] = 0\n",
    "for i in range(1,num_folds+1):\n",
    "    test['target_et_'+str(i)] = test['target_et_'+str(i)].rank(pct=True)\n",
    "    test['target'] = test['target'] + test['target_et_'+str(i)]\n",
    "test['target'] = test['target']/float(num_folds)\n",
    "test[['enrollee_id','target']].to_csv('sub_sam_et.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>target</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.905757</td>\n",
       "      <td>0.974945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <td>0.905757</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.902067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <td>0.974945</td>\n",
       "      <td>0.902067</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          target    target    target\n",
       "target  1.000000  0.905757  0.974945\n",
       "target  0.905757  1.000000  0.902067\n",
       "target  0.974945  0.902067  1.000000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ensemble\n",
    "sub1 = pd.read_csv('sub_sam_lgb.csv') #0.6892\n",
    "#sub2 = pd.read_csv('sub_sam_xgb.csv') #0.6871\n",
    "sub3 = pd.read_csv('sub_sam_rf.csv') #0.6871\n",
    "sub4 = pd.read_csv('sub_sam_et.csv') #0.6878\n",
    "#sub5 = pd.read_csv('sub_sam_cb.csv') #0.6832\n",
    "pd.concat([sub1.target,sub3.target,sub4.target],axis=1).corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test['target'] = (0.5*sub1.target + 0.35*sub3.target + 0.15*sub4.target)\n",
    "test[['enrollee_id','target']].to_csv('sub_sam_lgb_rf_et_ensemble.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
